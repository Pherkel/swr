{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# if this does not work, just set device to 'cpu'\n",
    "device = torch.device('mps' if torch.has_mps else 'cpu')\n",
    "# device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntroModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    This is a sample classe for SWR2.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear_mapping = torch.nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, input_, *args):\n",
    "        output = self.linear_mapping(input_)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.9587, -0.3237], device='mps:0', grad_fn=<LinearBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_model = IntroModel(1, 2).to(device)\n",
    "xx = torch.tensor([1.0], device=device)\n",
    "\n",
    "intro_model.forward(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx = torch.tensor([1.0, 2.0, 3.0, 4.0], device=device)\n",
    "\n",
    "#intro_model.forward(xx)  # RuntimeError\n",
    "\n",
    "xx.shape\n",
    "\n",
    "xx = xx.view(4, 1)\n",
    "\n",
    "intro_model.forward(xx)\n",
    "\n",
    "# now it works and gives the four results in parrallel\n",
    "\n",
    "yy = intro_model.forward(xx)\n",
    "\n",
    "# The first dimension is interpreted as batch dimension and all computatoins\n",
    "# are done in parralel along the first dimension.\n",
    "\n",
    "yy.shape  # [4, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0869],\n",
       "        [0.5548]], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The linear_mapping maps one number on two numbers. This is done by a linear\n",
    "# mapping with four paramerters. The four parameters are two weights and two\n",
    "# biases.\n",
    "\n",
    "intro_model.linear_mapping.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([ 0.8718, -0.8785], device='mps:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_model.linear_mapping.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True], device='mps:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The linear mapping multiplies the weights and adds the bias.\n",
    "xx = torch.tensor([3.1415], device=device)\n",
    "\n",
    "yy0 = intro_model.linear_mapping.weight[0] * xx + intro_model.linear_mapping.bias[0]\n",
    "yy1 = intro_model.linear_mapping.weight[1] * xx + intro_model.linear_mapping.bias[1]\n",
    "\n",
    "yy = intro_model.forward(xx)\n",
    "\n",
    "yy == torch.cat((yy0, yy1))  # they are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to change the behavior of the intro_model, so that it produces\n",
    "# two times the input as the first number and the negative input + 3 as the\n",
    "# second number.\n",
    "\n",
    "yy_true = torch.tensor([2 * xx, -xx + 3], device=device)\n",
    "\n",
    "# now we let the intro_model predict the (wrong) numbers\n",
    "yy = intro_model.forward(xx)\n",
    "\n",
    "# calculate an error\n",
    "error = torch.mean((yy_true - yy) ** 2)\n",
    "\n",
    "# backpropagete the error to get gradients on all weights and biases\n",
    "error.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-16.1418],\n",
       "        [  3.1597]], device='mps:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_model.linear_mapping.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-5.1382,  1.0058], device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_model.linear_mapping.bias.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The gradients give us the information how we have to change the weights and\n",
    "# biases to minimize the resulting error. We will change the weights and biases\n",
    "# only a little bit. This is called the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "new_weights = intro_model.linear_mapping.weight - learning_rate * intro_model.linear_mapping.weight.grad\n",
    "new_biases = intro_model.linear_mapping.bias - learning_rate * intro_model.linear_mapping.bias.grad\n",
    "\n",
    "intro_model.linear_mapping.weight = torch.nn.Parameter(new_weights)\n",
    "intro_model.linear_mapping.bias = torch.nn.Parameter(new_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in epoch 0 is: 37.04347610473633\n",
      "Error in epoch 1 is: 29.428565979003906\n",
      "Error in epoch 2 is: 23.3790283203125\n",
      "Error in epoch 3 is: 18.573074340820312\n",
      "Error in epoch 4 is: 14.755064010620117\n",
      "Error in epoch 5 is: 11.72191333770752\n",
      "Error in epoch 6 is: 9.312274932861328\n",
      "Error in epoch 7 is: 7.397978782653809\n",
      "Error in epoch 8 is: 5.877198696136475\n",
      "Error in epoch 9 is: 4.669041633605957\n",
      "Error in epoch 10 is: 3.7092416286468506\n",
      "Error in epoch 11 is: 2.946743965148926\n",
      "Error in epoch 12 is: 2.3409907817840576\n",
      "Error in epoch 13 is: 1.8597605228424072\n",
      "Error in epoch 14 is: 1.4774566888809204\n",
      "Error in epoch 15 is: 1.173740029335022\n",
      "Error in epoch 16 is: 0.9324582815170288\n",
      "Error in epoch 17 is: 0.7407752275466919\n",
      "Error in epoch 18 is: 0.5884963274002075\n",
      "Error in epoch 19 is: 0.4675213396549225\n",
      "Error in epoch 20 is: 0.37141457200050354\n",
      "Error in epoch 21 is: 0.2950640320777893\n",
      "Error in epoch 22 is: 0.23440854251384735\n",
      "Error in epoch 23 is: 0.18622197210788727\n",
      "Error in epoch 24 is: 0.1479407548904419\n",
      "Error in epoch 25 is: 0.11752899736166\n",
      "Error in epoch 26 is: 0.09336892515420914\n",
      "Error in epoch 27 is: 0.0741754025220871\n",
      "Error in epoch 28 is: 0.0589272640645504\n",
      "Error in epoch 29 is: 0.04681386053562164\n"
     ]
    }
   ],
   "source": [
    "# Now lets do this 10 times in a loop and see if the error gets smaller:\n",
    "\n",
    "for epoch in range(30):\n",
    "    yy = intro_model.forward(xx)\n",
    "    error = torch.mean((yy_true - yy) ** 2)\n",
    "    error.backward()\n",
    "    new_weights = intro_model.linear_mapping.weight - learning_rate * intro_model.linear_mapping.weight.grad\n",
    "    new_biases = intro_model.linear_mapping.bias - learning_rate * intro_model.linear_mapping.bias.grad\n",
    "\n",
    "    intro_model.linear_mapping.weight = torch.nn.Parameter(new_weights)\n",
    "    intro_model.linear_mapping.bias = torch.nn.Parameter(new_biases)\n",
    "\n",
    "    print(f\"Error in epoch {epoch} is: {float(error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m xx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1.5\u001b[39m])\n\u001b[1;32m      5\u001b[0m yy_true \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m xx, \u001b[39m-\u001b[39mxx \u001b[39m+\u001b[39m \u001b[39m3\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m yy \u001b[39m=\u001b[39m intro_model\u001b[39m.\u001b[39;49mforward(xx)\n\u001b[1;32m      7\u001b[0m error \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean((yy_true \u001b[39m-\u001b[39m yy) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mError: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mfloat\u001b[39m(error)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mIntroModel.forward\u001b[0;34m(self, input_, *args)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_, \u001b[39m*\u001b[39margs):\n\u001b[0;32m---> 11\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_mapping(input_)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/swr-4J-wxfLf-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/swr-4J-wxfLf-py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# The model can now create the desired output for this single input xx =\n",
    "# 3.1415, but what happens if we give it a new input?\n",
    "\n",
    "xx = torch.tensor([1.5])\n",
    "yy_true = torch.tensor([2 * xx, -xx + 3])\n",
    "yy = intro_model.forward(xx)\n",
    "error = torch.mean((yy_true - yy) ** 2)\n",
    "\n",
    "print(f\"Error: {float(error)}\")\n",
    "# For this new number we have a huge error :-("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in epoch 0 is: 0.5558255314826965\n",
      "Error in epoch 1 is: 0.5202839374542236\n",
      "Error in epoch 2 is: 0.48701515793800354\n",
      "Error in epoch 3 is: 0.4558735489845276\n",
      "Error in epoch 4 is: 0.42672333121299744\n",
      "Error in epoch 5 is: 0.3994370698928833\n",
      "Error in epoch 6 is: 0.3738956153392792\n",
      "Error in epoch 7 is: 0.349987268447876\n",
      "Error in epoch 8 is: 0.32760781049728394\n",
      "Error in epoch 9 is: 0.3066592812538147\n",
      "Error in epoch 10 is: 0.28705033659935\n",
      "Error in epoch 11 is: 0.26869526505470276\n",
      "Error in epoch 12 is: 0.25151386857032776\n",
      "Error in epoch 13 is: 0.23543114960193634\n",
      "Error in epoch 14 is: 0.22037681937217712\n",
      "Error in epoch 15 is: 0.20628505945205688\n",
      "Error in epoch 16 is: 0.193094402551651\n",
      "Error in epoch 17 is: 0.18074722588062286\n",
      "Error in epoch 18 is: 0.16918955743312836\n",
      "Error in epoch 19 is: 0.1583709567785263\n",
      "Error in epoch 20 is: 0.14824417233467102\n",
      "Error in epoch 21 is: 0.1387648731470108\n",
      "Error in epoch 22 is: 0.12989172339439392\n",
      "Error in epoch 23 is: 0.12158595770597458\n",
      "Error in epoch 24 is: 0.11381127685308456\n",
      "Error in epoch 25 is: 0.10653373599052429\n",
      "Error in epoch 26 is: 0.09972155839204788\n",
      "Error in epoch 27 is: 0.093345046043396\n",
      "Error in epoch 28 is: 0.087376169860363\n",
      "Error in epoch 29 is: 0.081789031624794\n"
     ]
    }
   ],
   "source": [
    "# But can we minimize the error for this values as well?\n",
    "\n",
    "for epoch in range(30):\n",
    "    yy = intro_model.forward(xx)\n",
    "    error = torch.mean((yy_true - yy) ** 2)\n",
    "    error.backward()\n",
    "    new_weights = intro_model.linear_mapping.weight - learning_rate * intro_model.linear_mapping.weight.grad\n",
    "    new_biases = intro_model.linear_mapping.bias - learning_rate * intro_model.linear_mapping.bias.grad\n",
    "\n",
    "    intro_model.linear_mapping.weight = torch.nn.Parameter(new_weights)\n",
    "    intro_model.linear_mapping.bias = torch.nn.Parameter(new_biases)\n",
    "\n",
    "    print(f\"Error in epoch {epoch} is: {float(error)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Error in epoch 0 is: 2.750251927238767e-11\n",
      "Average Error in epoch 1 is: 2.751284852720026e-11\n",
      "Average Error in epoch 2 is: 2.7550069820955758e-11\n",
      "Average Error in epoch 3 is: 2.7818884262148512e-11\n",
      "Average Error in epoch 4 is: 2.774258666543572e-11\n",
      "Average Error in epoch 5 is: 2.774731781146622e-11\n",
      "Average Error in epoch 6 is: 2.7797107723243066e-11\n",
      "Average Error in epoch 7 is: 2.7749538899363158e-11\n",
      "Average Error in epoch 8 is: 2.7723530120898897e-11\n",
      "Average Error in epoch 9 is: 2.7840594708089528e-11\n",
      "Average Error in epoch 10 is: 2.7761359062267176e-11\n",
      "Average Error in epoch 11 is: 2.7710927233415285e-11\n",
      "Average Error in epoch 12 is: 2.7640724084565526e-11\n",
      "Average Error in epoch 13 is: 2.7910725258761814e-11\n",
      "Average Error in epoch 14 is: 2.764221245996068e-11\n",
      "Average Error in epoch 15 is: 2.776451664063262e-11\n",
      "Average Error in epoch 16 is: 2.7698731971254052e-11\n",
      "Average Error in epoch 17 is: 2.7540555452496006e-11\n",
      "Average Error in epoch 18 is: 2.7933114640776856e-11\n",
      "Average Error in epoch 19 is: 2.7693502283043792e-11\n",
      "Average Error in epoch 20 is: 2.7702174408561488e-11\n",
      "Average Error in epoch 21 is: 2.788091120822789e-11\n",
      "Average Error in epoch 22 is: 2.7791700069751402e-11\n",
      "Average Error in epoch 23 is: 2.7856549827259824e-11\n",
      "Average Error in epoch 24 is: 2.759059040263745e-11\n",
      "Average Error in epoch 25 is: 2.7514616158380578e-11\n",
      "Average Error in epoch 26 is: 2.7667010436133178e-11\n",
      "Average Error in epoch 27 is: 2.781640588006562e-11\n",
      "Average Error in epoch 28 is: 2.7661119523375576e-11\n",
      "Average Error in epoch 29 is: 2.7651300207875984e-11\n",
      "Parameter containing:\n",
      "tensor([[ 2.0000],\n",
      "        [-1.0000]], device='mps:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.7700e-08, 3.0000e+00], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# The problem is that this might increase the error for the first number again.\n",
    "# Solution: Let us do it for many numbers over and over again.\n",
    "\n",
    "import numpy as np\n",
    "xxs = np.array(np.random.normal(size=100), dtype=np.float32)\n",
    "\n",
    "for epoch in range(30):\n",
    "    np.random.shuffle(xxs)  # we don't want to have the same order in each epoch\n",
    "    errors = list()\n",
    "    for xx in xxs:\n",
    "        xx = torch.tensor([xx], device=device)\n",
    "        yy_true = torch.tensor([2 * xx, -xx + 3], device=device)\n",
    "        yy = intro_model.forward(xx)\n",
    "        error = torch.mean((yy_true - yy) ** 2)\n",
    "        error.backward()\n",
    "        new_weights = intro_model.linear_mapping.weight - learning_rate * intro_model.linear_mapping.weight.grad\n",
    "        new_biases = intro_model.linear_mapping.bias - learning_rate * intro_model.linear_mapping.bias.grad\n",
    "\n",
    "        intro_model.linear_mapping.weight = torch.nn.Parameter(new_weights)\n",
    "        intro_model.linear_mapping.bias = torch.nn.Parameter(new_biases)\n",
    "        errors.append(float(error))\n",
    "    print(f\"Average Error in epoch {epoch} is: {np.mean(errors)}\")\n",
    "    \n",
    "print(intro_model.linear_mapping.weight)\n",
    "print(intro_model.linear_mapping.bias)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you find a -1, 0, 2, and 3 in the weights and biases? Where are these four numbers present as well?\n",
    "\n",
    "Yes, you can find these numbers. 2 and -1 correspond to the weights that are multiplied with the first and second input variable respectively\n",
    "0 and 3 are the biases that are added to the first and second variable\n",
    "this corresponds to the formula that we wanted to learn from the beginngin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a sencond intro_model that takes a vector with two numbers as\n",
    "# input and outputs a single number. Train the second intro model to produce\n",
    "# the sum of the two numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error: 6.295054912567139\n"
     ]
    }
   ],
   "source": [
    "xx = torch.tensor([1.0, 2.0], device=device)\n",
    "\n",
    "yy_true = torch.tensor([3.0], device=device)\n",
    "\n",
    "intro_model2 = IntroModel(2, 1).to(device)\n",
    "\n",
    "yy = intro_model2(xx)\n",
    "\n",
    "error = torch.mean((yy_true - yy) ** 2)\n",
    "\n",
    "print(f\"Initial error: {float(error)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I tried to optimize this code to run faster :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Error in epoch 0 is: 1.8546131372451782\n",
      "Average Error in epoch 1 is: 1.2377862215042115\n",
      "Average Error in epoch 2 is: 0.8261062681674958\n",
      "Average Error in epoch 3 is: 0.5514087736606598\n",
      "Average Error in epoch 4 is: 0.36810345351696017\n",
      "Average Error in epoch 5 is: 0.2457317441701889\n",
      "Average Error in epoch 6 is: 0.1640622526407242\n",
      "Average Error in epoch 7 is: 0.10954461023211479\n",
      "Average Error in epoch 8 is: 0.07314482554793358\n",
      "Average Error in epoch 9 is: 0.048846646770834924\n",
      "Average Error in epoch 10 is: 0.03262313194572926\n",
      "Average Error in epoch 11 is: 0.021789374575018883\n",
      "Average Error in epoch 12 is: 0.014554657507687807\n",
      "Average Error in epoch 13 is: 0.009722485952079297\n",
      "Average Error in epoch 14 is: 0.006495724432170391\n",
      "Average Error in epoch 15 is: 0.004339845711365342\n",
      "Average Error in epoch 16 is: 0.0028999084141105415\n",
      "Average Error in epoch 17 is: 0.0019378058612346649\n",
      "Average Error in epoch 18 is: 0.0012951020733453333\n",
      "Average Error in epoch 19 is: 0.000865473150042817\n",
      "Average Error in epoch 20 is: 0.0005785092071164399\n",
      "Average Error in epoch 21 is: 0.00038669714704155924\n",
      "Average Error in epoch 22 is: 0.00025849673693301156\n",
      "Average Error in epoch 23 is: 0.00017283222696278243\n",
      "Average Error in epoch 24 is: 0.00011554763914318755\n",
      "Average Error in epoch 25 is: 7.726618350716308e-05\n",
      "Average Error in epoch 26 is: 5.16704185429262e-05\n",
      "Average Error in epoch 27 is: 3.455446385487448e-05\n",
      "Average Error in epoch 28 is: 2.311106745764846e-05\n",
      "Average Error in epoch 29 is: 1.545940385767608e-05\n",
      "Average Error in epoch 30 is: 1.0340510198147967e-05\n",
      "Average Error in epoch 31 is: 6.917263362993253e-06\n",
      "Average Error in epoch 32 is: 4.627999987860676e-06\n",
      "Average Error in epoch 33 is: 3.0965371024649356e-06\n",
      "Average Error in epoch 34 is: 2.0721083728858503e-06\n",
      "Average Error in epoch 35 is: 1.3866667586626137e-06\n",
      "Average Error in epoch 36 is: 9.28070198824571e-07\n",
      "Average Error in epoch 37 is: 6.211109905507328e-07\n",
      "Average Error in epoch 38 is: 4.156913433916998e-07\n",
      "Average Error in epoch 39 is: 2.782998208772369e-07\n",
      "Average Error in epoch 40 is: 1.8631119758083514e-07\n",
      "Average Error in epoch 41 is: 1.2475417250357167e-07\n",
      "Average Error in epoch 42 is: 8.353992342335914e-08\n",
      "Average Error in epoch 43 is: 5.5955071687208147e-08\n",
      "Average Error in epoch 44 is: 3.749817665266164e-08\n",
      "Average Error in epoch 45 is: 2.5117657997952848e-08\n",
      "Average Error in epoch 46 is: 1.6834441129986998e-08\n",
      "Average Error in epoch 47 is: 1.1279133005359655e-08\n",
      "Average Error in epoch 48 is: 7.560191894739887e-09\n",
      "Average Error in epoch 49 is: 5.064557973000205e-09\n",
      "Parameter containing:\n",
      "tensor([[0.9999, 1.0000]], device='mps:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.7792e-05], device='mps:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# assuming that we have to train the model for a few epochs to learn the new formula\n",
    "xxs = np.array(np.random.normal(size=(10000, 2)), dtype=np.float32)\n",
    "xxs_tensor = torch.from_numpy(xxs).to(device)\n",
    "batch_size = 1000\n",
    "num_batches = len(xxs_tensor) // batch_size\n",
    "for epoch in range(50):\n",
    "    xxs_tensor = xxs_tensor[torch.randperm(len(xxs_tensor))]  # we don't want to have the same order in each epoch\n",
    "    errors = list()\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        xx_batch = xxs_tensor[batch_start:batch_end]\n",
    "        yy_true_batch = xx_batch.sum(dim=1, keepdim=True)\n",
    "        yy_batch = intro_model2.forward(xx_batch)\n",
    "        error_batch = torch.mean((yy_true_batch - yy_batch) ** 2)\n",
    "        error_batch.backward()\n",
    "        learning_rate = 0.01\n",
    "        new_weights = intro_model2.linear_mapping.weight - learning_rate * intro_model2.linear_mapping.weight.grad\n",
    "        new_biases = intro_model2.linear_mapping.bias - learning_rate * intro_model2.linear_mapping.bias.grad\n",
    "\n",
    "        intro_model2.linear_mapping.weight = torch.nn.Parameter(new_weights)\n",
    "        intro_model2.linear_mapping.bias = torch.nn.Parameter(new_biases)\n",
    "        errors.append(float(error_batch))\n",
    "    print(f\"Average Error in epoch {epoch} is: {np.mean(errors)}\")\n",
    "    \n",
    "print(intro_model2.linear_mapping.weight)\n",
    "print(intro_model2.linear_mapping.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swr-4J-wxfLf-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
